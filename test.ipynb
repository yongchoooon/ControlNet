{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 23:02:00.439553: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging improved.\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Loaded model config from [./models/cldm_v21.yaml]\n",
      "Loaded state_dict from [./lightning_logs/version_1/checkpoints/epoch=45-step=205620.ckpt]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]Global seed set to 1386433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 112), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:19<00:00,  2.61it/s]\n",
      " 14%|█▍        | 1/7 [00:19<01:58, 19.75s/it]Global seed set to 1386433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 112), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:18<00:00,  2.67it/s]\n",
      " 29%|██▊       | 2/7 [00:38<01:36, 19.30s/it]Global seed set to 1386433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 112), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:19<00:00,  2.63it/s]\n",
      " 43%|████▎     | 3/7 [00:58<01:17, 19.31s/it]Global seed set to 1386433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 112), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:19<00:00,  2.60it/s]\n",
      " 57%|█████▋    | 4/7 [01:17<00:58, 19.38s/it]Global seed set to 1386433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 112), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:19<00:00,  2.59it/s]\n",
      " 71%|███████▏  | 5/7 [01:37<00:38, 19.44s/it]Global seed set to 1386433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 112), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:19<00:00,  2.59it/s]\n",
      " 86%|████████▌ | 6/7 [01:56<00:19, 19.48s/it]Global seed set to 1386433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 112), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:19<00:00,  2.58it/s]\n",
      "100%|██████████| 7/7 [02:16<00:00, 19.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from share import *\n",
    "import config\n",
    "\n",
    "import cv2\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from annotator.util import resize_image, HWC3\n",
    "from annotator.midas import MidasDetector\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from cldm.ddim_hacked import DDIMSampler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "apply_midas = MidasDetector()\n",
    "\n",
    "model = create_model('./models/cldm_v21.yaml').cpu()\n",
    "model.load_state_dict(load_state_dict('./lightning_logs/version_1/checkpoints/epoch=45-step=205620.ckpt', location='cuda'))\n",
    "model = model.cuda()\n",
    "ddim_sampler = DDIMSampler(model)\n",
    "\n",
    "\n",
    "def process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, bg_threshold):\n",
    "    with torch.no_grad():\n",
    "        input_image = HWC3(input_image) # 이미지 전처리\n",
    "        \n",
    "       # _, detected_map = apply_midas(resize_image(input_image, detect_resolution), bg_th=bg_threshold)\n",
    "       # detected_map = HWC3(input_image)  \n",
    "        img = resize_image(input_image, image_resolution)\n",
    "        H, W, C = img.shape\n",
    "\n",
    "       # detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR) \n",
    "        input_image = cv2.resize(input_image, (W, H), interpolation=cv2.INTER_LINEAR) \n",
    "    \n",
    "    #   input_image = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR) \n",
    "\n",
    "       # control = torch.from_numpy(detected_map[:, :, ::-1].copy()).float().cuda() / 255.0\n",
    "        control = torch.from_numpy(input_image[:, :, ::-1].copy()).float().cuda() / 255.0\n",
    "        control = torch.stack([control for _ in range(num_samples)], dim=0)\n",
    "        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n",
    "\n",
    "        if seed == -1:\n",
    "            seed = random.randint(0, 65535)\n",
    "        seed_everything(seed)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n",
    "        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n",
    "        shape = (4, H // 8, W // 8)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=True)\n",
    "\n",
    "        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)  # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n",
    "        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n",
    "                                                     shape, cond, verbose=False, eta=eta,\n",
    "                                                     unconditional_guidance_scale=scale,\n",
    "                                                     unconditional_conditioning=un_cond)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        x_samples = model.decode_first_stage(samples)\n",
    "        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        results = [x_samples[i] for i in range(num_samples)]\n",
    "    return results\n",
    "\n",
    "def load_image_to_ndarray(file_path):\n",
    "    try:\n",
    "        pil_image = Image.open(file_path)\n",
    "        image_array = np.array(pil_image)\n",
    "        return image_array\n",
    "    except IOError:\n",
    "        print(f\"Unable to load the image from file: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "# Parameters\n",
    "a_prompt = 'best quality, extremely detailed'\n",
    "n_prompt = 'longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality'\n",
    "num_samples = 1\n",
    "image_resolution = 512\n",
    "detect_resolution = 384\n",
    "ddim_steps = 50\n",
    "guess_mode = False\n",
    "strength = 1.0\n",
    "scale = 9.0\n",
    "seed = random.randint(1000000, 9999999)\n",
    "eta = 0.0\n",
    "bg_threshold = 0.4\n",
    "\n",
    "# Inputs\n",
    "relative_path = '../fire_gen/fire_and_masked_images_30_real_new/train/masked/'\n",
    "file_list = ['S3-N1407MF01125.jpg', 'S3-N1411MF00607.jpg', 'S3-N1409MF01699.jpg', 'S3-N0886MF00021.jpg', 'S3-N1411MF00062.jpg', 'S3-N0840MF01423.jpg', 'S3-N0903MF01373.jpg']\n",
    "image_file_list = sorted([relative_path + i for i in file_list])\n",
    "\n",
    "prompt_list = ['A house is situated with an outdoor fire burning on firewood in front of it.',\n",
    "               'Near a house with white and black walls, there is a small orange flame flickering in the grass.',\n",
    "               'In close proximity to the grass, there stands an old house with a combination of gray and white colors.,',\n",
    "               'Some trees and bushes form the background as a large pile of dirt sits on the side of a road.',\n",
    "               'Surrounded by trees, a small gray cabin with a cozy fire burning inside provides a picturesque scene.',\n",
    "               'Next to the wall of a brick house, a small fire is smoldering, emitting a faint glow.',\n",
    "               'A hillside is visible in the background of the construction site.'\n",
    "               ]\n",
    "\n",
    "output_dir = 'inference_fire'\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "for i in tqdm(range(len(image_file_list))):\n",
    "    image_file_path = image_file_list[i]\n",
    "    input_image = load_image_to_ndarray(image_file_path)\n",
    "    prompt = prompt_list[i]\n",
    "\n",
    "    infer = process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, bg_threshold)\n",
    "\n",
    "    pil_image = Image.fromarray(infer[0])\n",
    "\n",
    "    image_file_name = image_file_path\n",
    "    pil_image.save(os.path.join(output_dir, image_file_path[-19:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "controlnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
